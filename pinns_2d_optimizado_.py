# -*- coding: utf-8 -*-
"""PINNs_2D_Optimizado .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QY5glNrRRNgsJdCsHM2RxJKzPzarwgkN
"""

#----------------------------------------------------------------------#
# Importación de bibliotecas necesarias para el modelo
#----------------------------------------------------------------------#
#-------------------------------
!pip install pyDOE                                # Instala pyDOE para usar Latin Hypercube Sampling

#-------------------------------
import tensorflow as tf                          # Librería principal para redes neuronales
import numpy as np                               # Operaciones numéricas con arreglos
from scipy.interpolate import griddata           # Interpolación para mapeo en una malla
import scipy.io                                  # Lectura y escritura de archivos .mat
from pyDOE import lhs                            # Latin Hypercube Sampling para generar puntos de entrenamiento
import time                                      # Para medir tiempos de ejecución
import math                                      # Funciones matemáticas estándar
#----------------------------------------------------------------------#

# Fijar semillas aleatorias para reproducibilidad
np.random.seed(1234)                             # Semilla para NumPy
tf.random.set_seed(1234)                         # Semilla para TensorFlow

# Variables de control (contadores)
Nfeval = 1                                       # Número de evaluación (por ejemplo, para mostrar la pérdida)
Nloss  = 0.0

#======================================================================#
# CLASE: Red Neuronal Informada por la Física (Physics-Informed NN)    #
#----------------------------------------------------------------------#
# Esta clase implementa una red neuronal para resolver la ecuación de  #
# Laplace en 2D usando el enfoque de PINNs                             #
# Incorpora condiciones de frontera y la ecuación diferencial como     #
# parte del proceso de entrenamiento.                                  #
#======================================================================#

class PhysicsInformedNN(tf.Module):

    # Constructor de la clase
    def __init__(self, X_u, u, X_f, rhs, layers):
        super().__init__()

        # Conversión de datos de entrada a tensores de TensorFlow
        #--------------------------------------------------------
        # Condiciones de frontera tipo Dirichlet:
        # Puntos (x, y) y sus valores u(x,y)
        self.xu = tf.convert_to_tensor(X_u[:, 0:1], dtype=tf.float32)
        self.yu = tf.convert_to_tensor(X_u[:, 1:2], dtype=tf.float32)
        self.u  = tf.convert_to_tensor(u, dtype=tf.float32)

        # Puntos de colocación internos (x, y) y el lado derecho de la EDP
        self.xf = tf.convert_to_tensor(X_f[:, 0:1], dtype=tf.float32)
        self.yf = tf.convert_to_tensor(X_f[:, 1:2], dtype=tf.float32)
        self.rhs = tf.convert_to_tensor(rhs, dtype=tf.float32)

        # Arquitectura de la red neuronal (lista de tamaños de capa)
        self.layers = layers

        # Inicialización de pesos y sesgos; creando el modelo
        self.model = self.create_model(layers)

        # Optimizador (Adam)
        self.optimizer = tf.keras.optimizers.Adam()

    #-------------------------------
    def create_model(self, layers):
      structure = []
      for N in layers[:-1]:
        structure.append(tf.keras.layers.Dense(N, activation = 'tanh'))
      structure.append(tf.keras.layers.Dense(1))

      model = tf.keras.Sequential(structure)
      return model

    #-------------------------------
    # Evaluación de la función
    def net_u(self, x, y):
        X = tf.concat([x, y], axis=1)  # Concatenación de coordenadas en un solo tensor
        return self.model(X)

    #-------------------------------
    # Red neuronal para evaluar la la EDP: f(x,y) = u_xx + u_yy
    def net_f(self, x, y):
        with tf.GradientTape(persistent=True) as tape2:     # Segundo orden: d²u/dx² y d²u/dy²
            tape2.watch([x, y])
            with tf.GradientTape(persistent=True) as tape1: # Primer orden: du/dx y du/dy
                tape1.watch([x, y])
                u = self.net_u(x, y)
            u_x = tape1.gradient(u, x)     # Derivada parcial de u respecto a x
            u_y = tape1.gradient(u, y)     # Derivada parcial de u respecto a y
        u_xx = tape2.gradient(u_x, x)      # Segunda derivada de u respecto a x
        u_yy = tape2.gradient(u_y, y)      # Segunda derivada de u respecto a y
        del tape1                          # Libera recursos
        del tape2
        return u_xx + u_yy                 # Evalúa el operador de Laplace (u_xx + u_yy)

    #-------------------------------
    # Función de pérdida total
    def loss_fn(self):
        u_pred = self.net_u(self.xu, self.yu)                   # Predicción de u en condiciones de frontera
        f_pred = self.net_f(self.xf, self.yf)                   # Predicción del residuo de la EDP

        loss_u = tf.reduce_mean(tf.square(self.u   - u_pred))   # Error en condiciones de frontera
        loss_f = tf.reduce_mean(tf.square(self.rhs - f_pred))   # Residuo de la EDP
        return loss_u + loss_f                                  # Pérdida total combinada

    #-------------------------------
    # Paso de entrenamiento optimizado
    @tf.function
    def train_step(self):
      with tf.GradientTape() as tape:                   # Graba operaciones para calcular gradientes
          loss_value = self.loss_fn()       # Evalúa la función de pérdida

      variables = self.model.trainable_variables             # Obtiene las variables (pesos y sesgos) entrenables del modelo
      grads     = tape.gradient(loss_value, variables)  # Calcula los gradientes de la pérdida respecto a las variables
      self.optimizer.apply_gradients(zip(grads, variables))  # Aplica los gradientes usando el optimizador
      return loss_value

    #-------------------------------
    # Proceso de entrenamiento
    def train(self, epochs=10000):
        for epoch in range(epochs):
            loss = self.train_step()  # Ejecuta un paso de entrenamiento
            if epoch % 100 == 0:      # Imprime cada 100 épocas
                print(f"Epoch {epoch}: Loss = {loss.numpy():.5e}")

    #-------------------------------
    # Predice u(x,y) para nuevos puntos
    def predict(self, X_star):
        x_star = tf.convert_to_tensor(X_star[:, 0:1], dtype=tf.float32)
        y_star = tf.convert_to_tensor(X_star[:, 1:2], dtype=tf.float32)
        return self.net_u(x_star, y_star).numpy()

#======================================================================#
#                        RUTINA PARA LA SOLUCIÓN                       #
# ---------------------------------------------------------------------#
# Este script entrena una red neuronal para resolver un problema       #
# elíptico bidimensional mediante el enfoque de PINNs. Se usan puntos  #
# de frontera para entrenar la condición de frontera y puntos de       #
# colocación aleatorios para imponer la PDE                            #
#======================================================================#

if __name__ == "__main__":

    #  .---------------------------------------------------------------.
    #  | PARÁMETROS:                                                   |
    #  | --------                                                      |
    #  | N_u    : Número de puntos de entrenamiento (frontera)         |
    #  | N_f    : Número de puntos de colocalización (interior)        |
    #  | --------                                                      |
    #  | Ne     : Número de neuronas por capa                          |
    #  | layers : Arquitectura de la red neuronal                      |
    #  | --------------------------------------------------------------|
    #  | MALLADO:                                                      |
    #  |                                                               |
    #  |                              [0:1,:]                          |
    #  |                        (xx1,uu1) <<u(-1,x)>>                  |
    #  |                                                               |
    #  |                    0   1   2  . . .Nx-3 Nx-2 Nx-1             |
    #  |              0   [ o   o   o  . . .  o   o   o ]              |
    #  |              1   [ o   *   *  . . .  *   *   o ]              |
    #  |  (xx2,uu2)   2   [ o   *   *  . . .  *   *   o ]   (xx3,uu3)  |
    #  | <<u(y,-1)>>  .   [ .   .   .         .   .   . ]   <<u(y,1)>> |
    #  |   [:,0:1]    .   [ .   .   .         .   .   . ]    [:,-1:]   |
    #  |              .   [ .   .   .         .   .   . ]              |
    #  |              Ny-3[ o   *   *  . . .  *   *   o ]              |
    #  |              Ny-2[ o   *   *  . . .  *   *   o ]              |
    #  |              Ny-1[ o   o   o  . . .  o   o   o ]              |
    #  |                                                               |
    #  |                              [-1,:]                           |
    #  |                       (xx4,uu4) <<u(1,x)>>                    |
    #  .---------------------------------------------------------------.

    # .================================================================.
    # | [S.1]                  PREPARACIÓN                             |
    # .================================================================.

    # .----------------------------------------------.
    # |                 PARÁMETROS                   |
    # .______________________________________________.

    #-------------------------------
    N_u = 40     # Puntos de entrenamiento en la frontera
    N_f = 1600   # Puntos de colocación en el dominio interior

    NN = 40
    Nx = NN + 1  # Número de nodos en x (malla para entrenamiento)
    Ny = NN + 1  # Número de nodos en y (malla para entrenamiento)

    Npx = NN + 1  # Nodos en x para predicción
    Npy = NN + 1  # Nodos en y para predicción

    Ne = 20        # Neuronas por capa oculta
    layers = [2, Ne, Ne, Ne, 1]  # Arquitectura: entrada, 3 ocultas, salida

    #-------------------------------
    # Mostrar resumen de parámetros en consola
    print('  ')
    print('---------------------------------------------')
    print('              PROBLEMA ELÍPTICO              ')
    print('---------------------------------------------')
    print('  ENTRENAMIENTO:                             ')
    print('  Puntos de frontera          N_u =', N_u)
    print('  Puntos de colocalización    N_f =', N_f)
    print('---------------------------------------------')
    print('  RED NEURONAL:                             ')
    print('  Capas =', layers)
    print('---------------------------------------------')
    print('  MALLA DE PREDICCIÓN:', Nx * Ny)
    print('  Nodos en dirección x     Nx =', Nx)
    print('  Nodos en dirección y     Ny =', Ny)
    print('---------------------------------------------')
    print('  ')

    # .----------------------------------------------.
    # |           MALLA: DOMINIO 2D                  |
    # .______________________________________________.

    # Definición del dominio: [-1, 1] x [-1, 1]
    xIni = -1.0; xFin = 1.0
    yIni = -1.0; yFin = 1.0

    # Malla para entrenamiento
    x = np.linspace(xIni, xFin, num=Nx)
    y = np.linspace(yIni, yFin, num=Ny)
    X, Y = np.meshgrid(x, y)

    # Malla para predicción
    xp = np.linspace(xIni, xFin, num=Npx)
    yp = np.linspace(yIni, yFin, num=Npy)
    XP, YP = np.meshgrid(xp, yp)

    # .----------------------------------------------.
    # |       SOLUCIÓN EXACTA (para referencia)      |
    # .______________________________________________.

    # Generar solución exacta en malla de entrenamiento
    Exact = np.zeros((Ny, Nx))
    for j in range(Ny):
        for i in range(Nx):
            Exact[j, i] = np.exp(x[i]) * np.cos(y[j])

    # Solución exacta en la malla de predicción
    UP_Exact = np.zeros((Npy, Npx))
    for j in range(Npy):
        for i in range(Npx):
            UP_Exact[j, i] = np.exp(xp[i]) * np.cos(yp[j])

    # .----------------------------------------------.
    # |     PUNTOS DE ENTRENAMIENTO Y COLOCACIÓN     |
    # .______________________________________________.

    # [0] Puntos de frontera (con valores exactos)
    xx1 = np.hstack((X[0:1, :].T, Y[0:1, :].T))     # y = -1
    uu1 = Exact[0:1, :].T
    xx2 = np.hstack((X[:, 0:1], Y[:, 0:1]))         # x = -1
    uu2 = Exact[:, 0:1]
    xx3 = np.hstack((X[:, -1:], Y[:, -1:]))         # x = 1
    uu3 = Exact[:, -1:]
    xx4 = np.hstack((X[-1:, :].T, Y[-1:, :].T))     # y = 1
    uu4 = Exact[-1:, :].T

    X_bdy = np.vstack([xx1, xx2, xx3, xx4])
    u_bdy = np.vstack([uu1, uu2, uu3, uu4])

    # [1] Puntos de colocalización en el dominio interior
    X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))
    lb = X_star.min(0)  # cota inferior
    ub = X_star.max(0)  # cota superior

    X_f = lb + (ub-lb)*lhs(2, N_f)  # LHS muestreo (Latin Hypercube)
    X_f_train = np.vstack(X_f)

    xf = X_f_train[:, 0:1]
    yf = X_f_train[:, 1:2]

    # Lado derecho: f(x,y) = 0
    rhs = np.zeros((N_f, 1))
    rhs_train = rhs

    # [2] Selección aleatoria de puntos de frontera para entrenar
    idx = np.random.choice(X_bdy.shape[0], N_u, replace=False)
    X_u_train = X_bdy[idx, :]
    u_train = u_bdy[idx, :]

    xu = X_u_train[:, 0]
    yu = X_u_train[:, 1]

    # [3] Puntos de predicción y solución exacta
    Xp_star = np.hstack((XP.flatten()[:, None], YP.flatten()[:, None]))
    up_star = UP_Exact.flatten()[:, None]

    # .================================================================.
    # | [S.2]                  APROXIMACIÓN                            |
    # .================================================================.

    # .----------------------------------------------.
    # |     ENTRENAMIENTO DEL MODELO (PINN)          |
    # .______________________________________________.

    #  Construcción de la red neuronal con condiciones físicas
    model = PhysicsInformedNN(X_u_train, u_train,
                              X_f_train, rhs_train,
                              layers)

    # Entrenamiento de la red neuronal
    start_time = time.time()
    model.train()
    elapsed = time.time() - start_time

    # .----------------------------------------------.
    # |         PREDICCIÓN SOBRE NUEVA MALLA         |
    # .______________________________________________.

    up_pred = model.predict(Xp_star)   # un solo vector de predicciones
    UP_pred = griddata(Xp_star, up_pred.flatten(), (XP, YP), method='cubic')  # 2D

    # .----------------------------------------------.
    # |     CÁLCULO DE ERRORES L2 Y MÁXIMO           |
    # .______________________________________________.

    error_u   = np.linalg.norm(up_star - up_pred, 2) / np.linalg.norm(up_star, 2)
    error_max = np.linalg.norm(up_star - up_pred, np.inf)

    # .================================================================.
    # | [S.3]              VISUALIZACIÓN DE RESULTADOS                 |
    # .================================================================.

    print('---------------------------------------------')
    print('  ENTRENAMIENTO:                             ')
    print('  Puntos de entrenamiento     N_u =', N_u)
    print('  Puntos de colocalización    N_f =', N_f)
    print('---------------------------------------------')
    print('  RED NEURONAL:                             ')
    print('  Capas =', layers)
    print('---------------------------------------------')
    print('  MALLA DE PREDICCIÓN:', Nx * Ny)
    print('  Nodos en dirección x     Nx =', Nx)
    print('  Nodos en dirección y     Ny =', Ny)
    print('---------------------------------------------')
    print('   N_u    N_f    Norma L2       Norma máx')
    print('  %3s   %3s   %e    %e' % (N_u, N_f, error_u, error_max))
    print('---------------------------------------------')
    print('  Tiempo de entrenamiento: %.4f segundos' % (elapsed))
    print('---------------------------------------------')
    print('  ')

#======================================================================#
#                 SUBRUTINA: Cargar y graficar datos                   #
#======================================================================#

import numpy as np                               # Cálculo numérico y manejo de arrays
import matplotlib.pyplot as plt                  # Generación de gráficos 2D
from mpl_toolkits.mplot3d import Axes3D          # Gráficos 3D con matplotlib

# .----------------------------------------------.
#  Graficar resultados
# .----------------------------------------------.
#  - Subplot 1: Solución analítica
#  - Subplot 2: Solución predicha
#  - Subplot 3: Distribución de puntos
fig = plt.figure(figsize=(12, 4))

# Subplot 1: Solución analítica
ax1 = fig.add_subplot(131, projection='3d')
Xp, Yp = np.meshgrid(xp, yp)

uA = up_pred.reshape(XP.shape)
ax1.plot_surface(Xp, Yp, UP_Exact, cmap='viridis')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('uA')
ax1.set_title('Solución analítica')

# Subplot 2: Solución estimada por la red
ax2 = fig.add_subplot(132, projection='3d')

Z_pred = model.predict(Xp_star)
ZP = Z_pred.reshape(XP.shape)

ax2.plot_surface(Xp, Yp, ZP, cmap='plasma')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_zlabel('uN')
ax2.set_title('Solución estimada')

# Subplot 3: Distribución de puntos de entrenamiento

ax3 = fig.add_subplot(133)
str1 = f'N_u={N_u}'
str2 = f'N_f={N_f}'

ax3.plot(xu, yu, 'ok', label=str1) # Boundry
ax3.plot(xf, yf, '.r', label=str2) # Latin HyperCube

ax3.set_xlabel('x')
ax3.set_ylabel('y')
ax3.set_title('Puntos en el dominio')
ax3.set_aspect('equal')
ax3.legend()
ax3.grid(True)


plt.tight_layout()
plt.show()